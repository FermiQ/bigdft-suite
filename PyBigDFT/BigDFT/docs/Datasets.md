# `PyBigDFT/BigDFT/Datasets.py`

## Overview

The `Datasets.py` module provides the `Dataset` class, which is designed to simplify the management and execution of a series of BigDFT calculations. It allows users to define a set of calculations with varying parameters, run them (potentially in parallel, depending on the underlying calculator), and then collect and process the results. This is particularly useful for studies involving parameter sweeps, convergence tests, or exploring different molecular configurations.

## Key Components

### `name_from_id(id: dict) -> str`
A utility function that generates a unique string name for a run based on its `id` dictionary. This helps in naming log files and other run-specific outputs.
-   `id`: A dictionary of key-value pairs that uniquely identifies a run (e.g., `{'hgrid': 0.4, 'crmult': 5}`).
-   Returns: A comma-separated string of `key:value` pairs.

### `class Dataset(Runner)`
Manages a collection of BigDFT calculations. It inherits from `Calculators.Runner`, allowing it to use a similar execution pattern (`pre_processing`, `process_run`, `post_processing`).
-   **`__init__(self, label: str = 'BigDFT dataset', run_dir: str = 'runs', **kwargs)`**:
    -   `label`: A descriptive label for the dataset.
    -   `run_dir`: The directory where individual runs will be performed.
    -   `**kwargs`: Additional global options passed to the underlying `Runner`. These can include default input parameters for all runs in the dataset.
-   **`append_run(self, id: dict, runner: Calculators.Runner, **kwargs)`**: Adds a new calculation (a "run") to the dataset.
    -   `id`: A dictionary uniquely identifying this run.
    -   `runner`: The calculator instance (e.g., `SystemCalculator`) to be used for this run.
    -   `**kwargs`: Run-specific input parameters that will be passed to the `runner.run()` method. These can override global options set during `Dataset` initialization (e.g., `input`, `posinp`).
-   **`process_run(self) -> dict`**: Executes all the appended runs. It iterates through the defined calculators and their associated runs, storing the results in `self.results`.
-   **`set_postprocessing_function(self, func: callable)`**: Sets a callback function that will be invoked after all runs in the dataset are complete. This function receives the `Dataset` instance itself as an argument.
    -   `func`: A callable that takes the `Dataset` instance as its argument.
-   **`post_processing(self, **kwargs) -> any`**: If a post-processing function is set via `set_postprocessing_function`, this method calls it. Otherwise, it returns the dictionary of results (`self.results`).
-   **`fetch_results(self, id: dict = None, attribute: str = None) -> list`**: Retrieves results from the completed runs.
    -   `id`: A dictionary to filter runs. Only runs whose `id` contains all items from this dictionary will be selected. If `None`, all results are considered.
    -   `attribute`: If specified, this attribute is fetched from each selected `Logfile` object (e.g., `'energy'`, `'forces'`). If `None`, the full `Logfile` objects (or other result objects from the calculator) are returned.
-   **Attributes**:
    -   `runs: list[dict]`: List of input parameters for each appended run, merged with global options.
    -   `calculators: list[dict]`: List of unique calculators and the indices of the runs they are responsible for.
    -   `results: dict`: Dictionary storing the results of each run, keyed by run index (0, 1, 2,...).
    -   `ids: list[dict]`: List of `id` dictionaries for each run.
    -   `names: list[str]`: List of unique string names for each run, generated by `name_from_id`.

### `combine_datasets(*args: Dataset) -> Dataset`
A utility function intended to combine multiple `Dataset` instances into a single, new `Dataset`.
*(Note: The current implementation of its post-processing logic (`_combined_postprocessing_functions`) is a placeholder (`pass`) in the source code.)*
-   `*args`: A variable number of `Dataset` objects to be combined.
-   Returns: A new `Dataset` instance containing all runs from the input datasets.

## Important Variables/Constants

No module-level variables are intended for direct external configuration.

## Usage Examples

```python
from PyBigDFT.BigDFT import Datasets, Calculators, Inputfiles
import os
import shutil

# Assume SystemCalculator is usable (BIGDFT_ROOT is set in the environment)
try:
    # Create a Dataset
    # Global options for all runs can be passed here, e.g., default input parameters
    study = Datasets.Dataset(label="H2_PES_Scan", run_dir="h2_pes_runs")

    # Define a base input (can also be passed to Dataset constructor)
    base_inp_dict = {"dft": {"ixc": "LDA", "hgrids": 0.4}}

    # Define different bond lengths for H2
    bond_lengths = [0.7, 0.74, 0.8, 0.9, 1.0] # Angstroem

    # Create a calculator instance to be used for the runs
    # This calculator's settings (omp, skip, etc.) will apply to runs using it.
    calc = Calculators.SystemCalculator(omp="1", skip=False, verbose=False)

    for i, length in enumerate(bond_lengths):
        run_id = {"bond_length_A": length} # Unique identifier for this run

        # Define positions for this specific run
        pos_dict = {
            "positions": [
                {"H": [0.0, 0.0, 0.0]},
                {"H": [0.0, 0.0, length]}
            ],
            "units": "angstroem"
        }

        # Prepare the full input for this run by combining base and specific parts
        # SystemCalculator's run method expects 'input' (dict) and 'posinp' (dict or str)
        # The Dataset's append_run will pass these kwargs to calc.run()
        # Note: SystemCalculator internally merges 'posinp' into the 'input' dict if 'posinp' is a dict.
        study.append_run(id=run_id,
                         runner=calc,
                         input=base_inp_dict, # Pass the DFT parameters
                         posinp=pos_dict)     # Pass the geometry

    # Define a post-processing function (optional)
    def analyze_results(dataset_instance):
        print(f"\nPost-processing dataset: {dataset_instance.global_options()['label']}")
        energies = []
        lengths = []
        # Iterate through results using the order of names/ids
        for i, run_name_str in enumerate(dataset_instance.names):
            log = dataset_instance.results.get(i) # Results are stored by index
            if log and hasattr(log, 'energy') and log.energy is not None:
                run_id_dict = dataset_instance.ids[i]
                lengths.append(run_id_dict["bond_length_A"])
                energies.append(log.energy)
                print(f"Run ID: {run_id_dict}, Name: {run_name_str}, Energy: {log.energy:.6f} Ha")
            elif log:
                print(f"Run ID: {dataset_instance.ids[i]}, Name: {run_name_str}, Energy not found or error in log.")
            else:
                print(f"Run ID: {dataset_instance.ids[i]}, Name: {run_name_str}, Logfile not found.")

        # Example: Plotting (requires matplotlib)
        # import matplotlib.pyplot as plt
        # if lengths and energies:
        #     plt.plot(lengths, energies, 'o-')
        #     plt.xlabel("Bond Length (A)")
        #     plt.ylabel("Energy (Ha)")
        #     plt.title(dataset_instance.global_options()['label'])
        #     # plt.show() # Uncomment to display plot
        #     print("Plotting example (code exists but display is commented out).")
        return {"bond_lengths": lengths, "energies": energies}

    study.set_postprocessing_function(analyze_results)

    # Run all calculations in the dataset
    print("Running dataset...")
    final_results_summary = study.run() # This will invoke the analyze_results function

    if final_results_summary:
        print("\nSummary from post-processing function:")
        for l, e in zip(final_results_summary.get("bond_lengths", []), final_results_summary.get("energies", [])):
            print(f"Length: {l} A, Energy: {e:.6f} Ha")

    # Example of fetching specific results after the run
    results_at_0_8A_energy = study.fetch_results(id={"bond_length_A": 0.8}, attribute="energy")
    if results_at_0_8A_energy:
        print(f"\nEnergy at 0.8 A (fetched): {results_at_0_8A_energy[0]:.6f} Ha")

    results_at_0_8A_log = study.fetch_results(id={"bond_length_A": 0.8})
    if results_at_0_8A_log and results_at_0_8A_log[0] is not None:
        print(f"Logfile object for 0.8 A run name: {results_at_0_8A_log[0].logname}")


except AssertionError as e:
    print(f"Skipping Datasets example: SystemCalculator requirement failed: {e}")
    print("Please ensure BIGDFT_ROOT is set in your environment and 'bigdft' executable is accessible.")
except Exception as e:
    print(f"An unexpected error occurred during Datasets example: {e}")
finally:
    # Clean up run directory
    if os.path.exists("h2_pes_runs"):
        shutil.rmtree("h2_pes_runs")
    # General cleanup for any stray files if direct calculator calls were made outside Dataset's run_dir
    # This part might be overly aggressive if other tests are running in parallel.
    # For a standalone example, it's fine.
    # files_to_remove_patterns = ["log-", "time-", ".yaml"] # Example patterns
    # dirs_to_remove_patterns = ["data-"]
    # for item in os.listdir("."):
    #     for pattern in files_to_remove_patterns:
    #         if item.startswith(pattern.split('*')[0]) and item.endswith(pattern.split('*')[-1]):
    #             if os.path.isfile(item): os.remove(item)
    #     for pattern in dirs_to_remove_patterns:
    #         if item.startswith(pattern.split('*')[0]) and item.endswith(pattern.split('*')[-1]):
    #             if os.path.isdir(item): shutil.rmtree(item)
    if os.path.exists("debug"): # Often created by BigDFT on error
        shutil.rmtree("debug")

```

## Dependencies and Interactions

-   **`Calculators.Runner`**: `Dataset` inherits from this class.
-   **`Calculators.SystemCalculator` or `Calculators.GIBinding`**: Instances of these (or other `Runner`-derived) calculators are passed to `append_run` to perform the actual calculations.
-   **`copy.deepcopy`**: Used for managing input dictionaries and options.
-   **`futile.Utils.make_dict`**: Used internally (though not explicitly imported in `Datasets.py`, it's a common utility in the ecosystem, and `Calculators.py` uses it).
-   **`BigDFT.Logfiles.Logfile`**: Results of runs performed by `SystemCalculator` are typically `Logfile` objects, which `Dataset` stores and `fetch_results` can access.
-   **`BigDFT.Inputfiles.Inputfile`**: Often used to prepare the input dictionaries for runs, as shown in the example.
```
