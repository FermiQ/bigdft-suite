<?xml version="1.0" encoding="iso-8859-15"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
  <head>
    <title>Tutorial: Acceleration example on different platforms</title>
    <meta name="copyright" content="&#169; 2004-2011 CEA" />
    <link rel="author" type="text/html" href="http://www.cea.fr" hreflang="fr" lang="fr" />
    <link rel="stylesheet" type="text/css" href="../styles/original.css" />
  </head>

  <body>

    <div class="header">
      <div class="visuLogo">
        <img src="../images/logo_header.png" alt="" width="102" height="80" />
      </div>
      <!-- %%VERSION%% -->
    </div>

    <!-- %%MENU%% -->

    <div class="main"><a name="top" id="top"></a>

      <div class="floatingmenu">
        <a href="H1A-Basics.html">&lt;&nbsp;First Group</a>
        <a style="float:right;" href="H2B-scalability.html">Next lesson &gt;</a> 
        <h2>Content of this lesson</h2>
        <ul>
	  <li>Compiling GPU versions of the BigDFT code</li>
          <li>Testing GPU routines</li>
	  <li>Doing total energy calculations on different platforms</li>
	  <li>Doing parallel computation using GPU version</li>
        </ul>
        <h2>Material</h2>
        <ul style="float:left;margin-right:1em;">
          <li>Si_periodic/<a href="Si_periodic/input.dft">input.dft</a></li>
          <li>Si_periodic/<a href="Si_periodic/posinp.xyz">posinp.xyz</a></li>
          <li>Si_periodic/<a href="Si_periodic/psppar.Si">psppar.Si</a></li>
        </ul>
        <ul style="float:left;margin-right:1em;">
          <li>Ca2/<a href="Ca2/input.dft">input.dft</a></li>
          <li>Ca2/<a href="Ca2/posinp.xyz">posinp.xyz</a></li>
          <li>Ca2/<a href="Ca2/psppar.Ca">psppar.Ca</a></li>
        </ul>
	<ul style="float:left;margin-right:1em;">
          <li>Si_large/<a href="Si_large/input.dft">input.dft</a></li>
          <li>Si_large/<a href="Si_large/posinp.xyz">posinp.xyz</a></li>
          <li>Si_large/<a href="Si_large/psppar.Si">psppar.Si</a></li>
        </ul>
      </div>

      <p class="warn">This lesson has been created for current stable version. Earlier
        versions are fully capable of running this tutorial but input files may
        have to be changed according to possible earlier formats.</p>

      <h1>Acceleration example on different platforms: OpenCL and CUDA version</h1>

      <p>The purpose of this lesson is to introduce the usage of the GPU version(s) of the BigDFT code. You will
	learn basics of making GPU runs and benefits of using GPU version.</p>

      <h2>Compiling GPU versions of the BigDFT code</h2>
      
      <p>Compilation of the OpenCL version of the code requires 
        <code>--enable-opencl</code> option in the configure sequence. Some other options like 
	<code>--with-ocl-path</code> may also be specified. See the example below (which is installation-specific):</p>

      <pre> &lt;path_to_distribution&gt;/configure FC=mpif90 FCFLAGS="-O2 -i_dynamic" 
	--with-ext-linalg="-lmkl_lapack95_lp64 -lmkl_intel_lp64  -lguide 
	-lmkl_intel_thread -lmkl_core -lpthread"  --enable-opencl</pre>

      <p>For the CUDA version <code>--enable-cuda-gpu</code> option should be used
	in the configure sequence. <code>--with-cuda-path</code> option may also be specified.</p>
      
	<pre> &lt;path_to_distribution&gt;/configure FC=mpif90 FCFLAGS="-O2 -i_dynamic" 
	--with-ext-linalg="-lmkl_lapack95_lp64 -lmkl_intel_lp64  -lguide 
	-lmkl_intel_thread -lmkl_core" --enable-cuda-gpu</pre>

	<div class="exercice">
        <p><b>Exercise</b>: Compile the OpenCL and and CUDA versions of the code in two 
	separate folders.</p>
	<p class="answer">Create a folder where you want to build the code and switch to that 	
	folder. Call the configure script in the BigDFT main source folder by specifying its 
	relative or absolute path. Add the options necessary for the version you 
	want to compile as described above. Check the informative messages at the end of the 
	configure to see if the OpenCL or CUDA was enabled or not. Specify OpenCL or CUDA path if 
	it is necessary by the additional options described above. If everything
	went okay in the configure you can compile the code by typing <code>make</code>.
	Not that, if your system supports it, you might create one single set of binaries valid for both for OpenCL and CUDA implementations.</p>
	 </div>

      <h2>Testing GPU routines</h2>
    
      <p><code>conv_check</code> routine makes checks on the GPU related parts of the code. You 	can compile and run it by typing <code>make check</code> in the <code>tests/CUDA</code>
	or <code>tests/OpenCL</code> subfolder of bigDFT build folder depending on the acceleration 
	type.</p>

      <div class="exercice">
        <p><b>Exercise</b>: Go to your CUDA build folder and switch to subfolder <code>tests/CUDA
	</code>. Type: <code>make check</code> to compile and run the <i>conv_check</i> 
	program. Observe the output which is generated in <i>conv_check.out</i> file.</p>
	<p class="answer">The output will have comparisons of CPU and GPU timings of code 
	sections which was implemented by CUDA:</p>
	<span class="override"><pre>CPU Convolutions, dimensions:   124 17160
GPU Convolutions, dimensions:   124 17160
 | CPU: ms  |  Gflops  || GPU:  ms |  GFlops  || Ratio  | No. Elements | Max. Diff. |
 |     25.84|      2.64||      1.07|     63.65||  24.150|       2127840|  2.2204E-16|</pre></span>
	<p class="answer">Computation time ratio of CPU and GPU versions are given as <code>
	Ratio</code>. Observe that the benefit of using GPU varies depending on the kernel which has been ported (and also from the machine).</p>  
      </div>

	<div class="exercice">
        <p><b>Exercise</b>: Go to your OpenCL build folder and switch to subfolder 
	<code>tests/OpenCL</code>. Type: <code>make check</code> to compile and run the 	
	<i>conv_check</i> program. Observe the output which is generated in <i>conv_check.out</i> 
	file.</p>
	<p class="answer">The output will have comparisons of CPU and GPU timings of code 
	sections which was implemented by OpenCL:</p>
	<span class="override"><pre>CPU Convolutions, dimensions:   124 17160
GPU Convolutions, dimensions:   124 17160
 | CPU: ms  |  Gflops  || GPU:  ms |  GFlops  || Ratio  | No. Elements | Max. Diff. |
 |     25.47|      2.67||      0.87|     78.24||  29.266|       2127840|  4.4409E-16|</pre></span>
	<p class="answer">Computation time ratio of CPU and GPU versions are given as <code>
        Ratio</code>. Observe that the OpenCL version has higher performance compared 
	to CUDA version for specific parts of the code and OpenCL version has more GPU enabled 
	code sections. Therefore, OpenCL version of the code should be preferred.
	In particular, the CUDA version of the code works only for fully periodic BC at gamma point only.
	It has therefore less functionalities and it is not maintained anymore.
	</p>  
      </div>

In order to test the behaviour of the complete code operations, a functionality is added in the <code>memguess</code> program.
This functionality, called <code>GPUtest</code> will run the BigDFT unitary 3D operations (Density construction, Local Hamiltonian and preconditioning) with and without GPU acceleration.
This is a useful tool to predict the advantages of GPU usage.
	<div class="exercice">
        <p><b>Exercise</b>: To activate the <code>GPUtest</code> you should call <code>memguess</code> this way:
	  <pre>
	    ../../../src/memguess 1 &lt;name&gt; GPUtest &lt;nrep&gt; &lt;norbs&gt; </pre>
	 where <code>&lt;name&gt;</code>, <code>&lt;nrep&gt;</code> and <code>&lt;norbs&gt;</code> are optional arguments indicating the run name, the number of repetitions of the calculation and the number of orbitals treated. their default values are <code>none</code>, <code>1</code> and <code>norb</code> (the total number of orbitals in the  system), respectively.
	<p class="answer">The output will have comparisons of CPU and GPU timings of code 
	sections which was implemented by OpenCL and CUDA:</p>
	<span class="override"><pre>
---------------------------------- CPU-GPU comparison: Density calculation
 | CPU: ms  |  Gflops  || GPU:  ms |  GFlops  || Ratio  | No. Elements | Max. Diff. |
 |     37.36|      2.63||      2.81|     34.98||  13.294|        512000|  9.7145E-17|
 ---------------------------------- CPU-GPU comparison: Local Hamiltonian calculation
 ekin,epot=   56.3306474633124        93.0696865423315     
 ekinGPU,epotGPU   56.3306474633134        93.0696865423313     
 | CPU: ms  |  Gflops  || GPU:  ms |  GFlops  || Ratio  | No. Elements | Max. Diff. |
 |    117.52|      2.28||      7.46|     35.98||  15.762|      65536000|  7.4772E-16|
 ---------------------------------- CPU-GPU comparison: Linear Algebra (Blas)
 | CPU: ms  |  Gflops  || GPU:  ms |  GFlops  || Ratio  | No. Elements | Max. Diff. |
 |   1789.95|      9.37||   1790.32|      9.37||   1.000|         16384|  0.0000E+00|
 | CPU: ms  |  Gflops  || GPU:  ms |  GFlops  || Ratio  | No. Elements | Max. Diff. |
 |   1315.30|      6.43||   1314.96|      6.43||   1.000|         16384|  0.0000E+00|
 ---------------------------------- CPU-GPU comparison: Preconditioner
 gnrm   284.904515990837     
 gnrmGPU   284.904515990839     
 | CPU: ms  |  Gflops  || GPU:  ms |  GFlops  || Ratio  | No. Elements | Max. Diff. |
 |    133.67|      4.10||     20.51|     26.72||   6.519|      65536000|  3.1919E-16|
 Ratios:  13.294  15.762   1.000   1.000   6.519
</pre></span>
	<p class="answer">
	  Run these features with the systems presented in these page, with and without CUDA (if available).
	  Experience Amdahl's law: compare the behaviours with the run of the whole code (see below).
	</p>  
	</div>

	

      <h2>Doing total energy calculations on different platforms</h2>
      
      <p>For both OpenCL and CUDA versions an additional input file <code>input.perf</code>
	should be provided for the BigDFT run. <br><br>

	For OpenCL version first line of this file 
	should have the keywords <code>ACCEL OCLGPU</code>. <br><br>

	For CUDA version the keywords should
	be <code>ACCEL CUDAGPU</code> and an additional file called 
	<code>GPU.config</code> should also be provided with some parameter values: </p>
      <pre>USE_SHARED=0

MPI_TASKS_PER_NODE=1
NUM_GPU=1

GPU_CPUS_AFF_0=0,1,2,3
GPU_CPUS_AFF_1=4,5,6,7

USE_GPU_BLAS=1
USE_GPU_CONV=1</pre>
      
	<p> Benefit of using GPU version depends on the system studied. If for a specific
	system, most of the computation time is spent in GPU enabled parts of the code, using GPU 
	version would be very beneficial. However, some system runs require most of the 
	computation to be done with the code sections not using GPU, such as the Poisson solver. 
	For such systems, GPU version would not be much beneficial. Two systems, first benefiting, 
	second not benefiting the GPU version are studied in the below two exercises.  
	</p>

	<div class="exercice">
        <p><b>Exercise</b>: Do total energy calculations for a periodic Si system using CPU, CUDA and OpenCL 
	versions of the code. You will need <a href="GPU/Si8.dft"><code>input.dft</code></a>
	and <a href="GPU/Si8.xyz"><code>posinp.xyz</code></a> for this computation.
	For the OpenCL and CUDA computations create the additional
	input file(s) described above. Compare computation times of different versions. Do 
	not use MPI or OpenMP parallelization for any
	runs in order to better observe the GPU acceleration benefits. The MPI and OpenMP parallelization schemes will be the objects of the next lesson. (Set <code>
	OMP_NUM_THREADS</code> environment parameter to 1 in order to disable OpenMP
	parallelization):
	<pre>export OMP_NUM_THREADS=1</pre></p>
	<p class="answer"> Redirect your output to a file and check computation time by the 
	command:
	<pre>grep time file.out</pre></p>
	<p class="answer">This command will give you <i>CPU time</i> and <i>elapsed time</i>. 
	<pre>CPU time/ELAPSED time for root process     0         8.26         7.43</pre></p>
	<p class="answer">Rely on CPU times for all cases. There should be a good benefit of
	using GPU for this periodic system since convolutions comprise most of the DFT
	calculation for this case.
	</p>
	In actual performance evaluations, the correct quantity which should be consideres is the <code>WFN_OPT</code> category in the <code>(data/)time.prc</code> file. Indeed, the initialization and finalization timings (including the timings needed to compile OpenCL kernels of for initialize <code>S_GPU</code> library in CUDA) are not representatives of actual performances.
So you should 
	<pre>grep WFN_OPT time.prc  | grep Total</pre>
to have, for example
	<pre>Total CPU time for category: WFN_OPT   =    5.10E+00      Total categorized percent  97.8</pre>
	</p>
      </div>

	<div class="exercice">
        <p><b>Exercise</b>: Do total energy calculations for Ca2 molecule in the
	<code>/tests/DFT/Ca2</code> folder of BigDFT source using CPU and OpenCL 
	versions of the code. You will need <a href="GPU/Ca2.dft"><code>input.dft</code></a>
	and <a href="GPU/Ca2.xyz"><code>posinp.xyz</code></a> files for this computation.
	For the OpenCL version create the additional <a href="GPU/Ca2.perf"><code>input.perf</code></a>
	input file described above. Compare computation times of the two versions
	as in the previous exercise. Do not use MPI or OpenMP parallelization also
	for this case.
	<p class="answer"> For this free boundary condition calculation, most
	of the computation time is spent in Poisson solver part of the code which
	does not facilitate GPU. Therefore, benefit of using GPU for this calculation
	would not be much.
	</p>
	</p>
      </div>

	 <h2>Doing parallel computation using GPU version</h2>

	<p>BigDFT GPU runs can benefit MPI parallelization. Multiple MPI processors can
	access to the same GPU card in such a run. However, benefit of using MPI parallelization
	would not be as much as pure MPI version since in general, on a given machine the number of MPI procs is bigger than the number of GPU cards thus several MPI procs have to share the same GPU.</p> 

	<div class="exercice">
        <p><b>Exercise</b>: Do total energy calculation for the periodic Si system in the
	<code>/tests/DFT/GPU</code> folder of BigDFT source using 4 MPI nodes with and without OpenCL
	acceleration. You will need 
	<a href="GPU/Si8.dft"><code>input.dft</code></a> and
	, <a href="GPU/Si8.xyz"><code>posinp.xyz</code></a> files for this computation.
	As the first step, compare the computation time of OpenCL+MPI version with the computation
	time of the single node OpenCL calculation. Then, compare the computation time of the pure MPI 
	run with the computation time of single node CPU calculation for this system. Use the results 
	of previous exercises for comparisons.
	</p>
	<p class="answer">Computation time of the 4 MPI node GPU calculation for this system
	would not necessarily be significantly less than the single node GPU calculation. 
	  However, this is of course due to the smallness of the system.</p> 
	</p>
      </div>

	<p>If there are more than one GPU cards in the compute platform then different
	MPI processes can use different GPU cards for better performance. For the CUDA version,
	number of GPU devices and number of MPI nodes should be specified in the <code>input.perf
	</code> file. OpenCL version detects and uses multiple GPU cards automatically.</p>

	<div class="exercice">
        <p><b>Example</b>: In this example we will do a calculation on a 64 atom preiodic Si
	system using OpenCL+MPI version of the code. We will need 
	<a href="GPU/Si64.dft"><code>input.dft</code></a> and
	, <a href="GPU/Si64.xyz"><code>posinp.xyz</code></a> files for this computation.
	 We will use a large cluster in which each workstation having a GPU card.
	Number of total MPI processes will be determined according
	to the available cluster. We will compare the computation time of OpenCL+MPI version
	with the computation time of the pure MPI version having same number of MPI processes. 
	</p>
	<p class="answer">In this run the OpenCL version of the code will arrange the GPU usage
	automatically. Each MPI process will access the GPU card on the same physical machine it
	runs. The speed up due to GPU usage would depend on the number of GPU cards (number of
	physical nodes) in the cluster. </p> 
	</p>
      </div>

    <div class="footer">Author (Nazim Dugan U. Basel ch)
      |
      <a href="http://validator.w3.org/check/referer" title="Check HTML compliance with W3C norms">XHTML1.0</a> - 
      <a href="http://jigsaw.w3.org/css-validator/check/referer" title="Check CSS compliance with W3C norms">CSS2</a>
      |
      <!-- hhmts start -->
      Last modified: Tue Oct 11 16:50:00 CEST 2011
      <!-- hhmts end -->
    </div>

  </body>
</html>

