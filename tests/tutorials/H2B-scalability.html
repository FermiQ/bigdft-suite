<?xml version="1.0" encoding="iso-8859-15"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
  <head>
    <title>Scalability of BigDFT with MPI and OpenMP</title>
    <meta name="copyright" content="&#169; 2004-2011 CEA" />
    <link rel="author" type="text/html" href="http://www.cea.fr" hreflang="fr" lang="fr" />
    <link rel="stylesheet" type="text/css" href="../styles/original.css" />
  </head>

  <body>

    <div class="header">
      <div class="visuLogo">
        <img src="../images/logo_header.png" alt="" width="102" height="80" />
      </div>
      <!-- %%VERSION%% -->
    </div>

    <!-- %%MENU%% -->

    <div class="main"><a name="top" id="top"></a>

      <div class="floatingmenu">
        <a href="H2A-GPU.html">&lt;&nbsp;prev. lesson</a>
        <a style="float:right;" href="H2E-minima_hopping.html">next lesson &gt;</a> 
        <h2>Content of this lesson</h2>
        <ul>
          <li>Introduction</li>
	  <li>Lesson details</li>
          <li>Discussions</li>
        </ul>
        <h2>Materials provided</h2>
        <ul>
          <li><a href="scalability/posinp.xyz" target="_blank">posinp.xyz</a></li>
          <li><a href="scalability/B14.dft" target="_blank">input.dft</a></li>
          <li><a href="scalability/run_job" target="_blank">run_jobs</a></li>
          <li><a href="scalability/times.bash" target="_blank">times.bash</a></li>
        </ul>
      </div>

      <h1>Scalability of BigDFT with MPI and OpenMP</h1>

      <h2>Introduction</h2>
      <img class="figure" src="Figures/B14_2.jpg" alt="visualisation of the Boron 14 cluster" width="180" />
      <p>There are two levels of parallelization implemented in BigDFT, a massively parallelized DFT code: MPI and OpenMP. 
      Of the two parallelizations, the former works on distributed memory architectures while the latter corresponds to shared 
      memory architectures. Each has some particular advantages as well as disadvantages. BigDFT can benefit from selected advantages
      of both MPI and OpenMP if a proper combination of the two is chosen.</p>

      <p>The MPI parallelization in BigDFT relies on the orbital distribution scheme, in which the orbitals of the system under 
      investigation are distributed over the assigned MPI processes. This scheme reaches its limit when the number of MPI processes 
      is equal to the number of orbitals in the simulation. To equally distribute the orbitals, the number of processors must be a factor
     (divisor) of the number of orbitals. If this is not the case, the distribution is not optimal, but BigDFT tries to balance the load
      over the processors. For example, if we have 5 orbitals and 4 processors, the orbitals will have the distribution: 2/1/1/1.</p>

     <p> Within an orbital, where the functionality is activated by the proper compiler option, BigDFT uses OpenMP to do the parallelization. 
      For a given MPI task, the most time consuming routines in BigDFT are parallelized by OpenMP, and they are indeed sped up very well. 
      There are however some parts of BigDFT which, because of some reasons, are not parallelized by OpenMP yet, and 
      further treatment for these parts is in progress.</p>

     <p>When a part of the code is parallelized by OpenMP, the corresponding work is performed in parallel 
     by some OpenMP threads. To obtain an optimal behaviour, for each MPI process we need to assign a number of cores (or cpus), one core per thread, that will be responsible for managing the OpenMP threads. 
     If we plan to use, for example, OpenMP with 6 threads, we have to assign at least 6 cores (cpus) to each MPI processes.</p>

<h2>Lesson details</h2>

 <h3>Material for the lesson</h3>

      <p>In this lesson, we will explain how to run BigDFT with MPI and/or OpenMP. We will show some examples of how BigDFT scales 
         with the number of MPI processes and OpenMP threads. We will also provide a small bash script to extract total or partial times. For this purpose, we will run BigDFT jobs for 
         a particular system, a Boron cluster with 14 atoms and 21 orbitals, shown in the top left figure. This system has been chosen just for the fact that it has a not "obvious" number of orbitals, so that obtaining an optimal speedup with a single node is almost impossible. The files which can be used are the following:</p>
      <ul>
         <li><a href="scalability/posinp.xyz" target="_blank">posinp.xyz</a>: the geometrical structure of the Boron 14 cluster, in format of an xyz file</li>
         <li><a href="scalability/B14.dft" target="_blank">input.dft</a>: the inputs for a high precision DFT run with BigDFT (chosen for having a big number of degrees of freedom</li>
         <li><a href="scalability/curie.sub" target="_blank">run_job</a>: the script to submit a BigDFT job with MPI and OpenMP</li>
      </ul>

 <h3>Job submission script</h3>

      <p>Since the format of the <code>posinp.xyz</code> and <code>input.dft</code> files has been discussed in previous lessons, we will have a 
         look only at the script for submitting a BigDFT job (<code>run_job</code>) with MPI and OpenMP. 
The given example is valid for for the CURIE machine, but of course is not general. For example, on Palu, a Cray XE6, it looks like:</p>
      <pre>
#!/bin/sh
#SBATCH --job-name="handson"
#SBATCH --ntasks=12
#SBATCH --ntasks-per-node=6
#SBATCH --cpus-per-task=2
#SBATCH --time=00:19:00
export OMP_NUM_THREADS=2
cd /users/huantd/handon
aprun -n 12 -N 6 -d 2 bigdft | tee Out_mpi12_omp2
      </pre>
<p>In general, the parameters which are relevant to MPI and OpenMP parallelization are</p>
<ul>
<li><b>ntasks</b>: Number of MPI processes requested for the job. Because the Boron 14 has 21 orbitals, if we choose ntasks=1, 
there is 1 MPI process which work with all 21 orbitals. On the other hand, if we choose ntasks=21, 
calculations related to each of the 21 orbitals of the cluster is performed on a given MPI process.</li>

<li><b>cpus-per-task</b>: Number of OpenMP threads we plan to use for OpenMP parallelization in the code (equivalent to <code>OMP_NUM_THREADS</code>) </li>
<li><b>ntasks-per-node</b>: Number of MPI processes to be run on one node. For example, because a node 
on palu machine has 24 cores (cpus), if we choose ntasks-per-node=6, we can specify cpus-per-task 
up to 4. In the CURIE machine script, this quantity is inferred from the number of nodes used for the calculation (<code>-N</code> option)</li>
<li><b>export OMP_NUM_THREADS=2</b>: OMP_NUM_THREADS is an environmental variable which have to be set to the 
number of OpenMP threads requested. In the submission scrip, we set OMP_NUM_THREADS=2, implying that 
we will run the job with 2 OpenMP threads when a segment of the code is parallelized by OpenMP.</li>
<li><b>aprun -n 12 -N 6 -d 2 bigdft | tee Out_mpi12_omp2</b>: the syntax to launch the bigdft excutable on 
Palu. Here, 12 is ntasks, 6 is ntasks-per-node, and 2 is cpus-per-task. This syntax may be different 
on different machine, e.g., aprun is replaced by mpirun on machines with mpich or openmpi. Each installation has its own submission script.</li>
</ul>

<p>After preparing the submission script, it is submitted by a given syntax, e.g.,</p>
<pre>
ccc_msub run_job
</pre>
or (in other systems)
<pre>
qsub run_job
</pre>

<h3>Timing data to be analyzed</h3>

<p>The timing data of a BigDFT job is saved in the file <code>time.prc</code>, so after a job ends, do not 
forget to rename this file to avoid it being overwritten. 
If other data have to be written, this file can be found in the <code>data/</code> directory. 
A quick look into the timing file can 
reveal the time needed for different categories of operations performed in the run. Below are the most time consuming 
categories for the B14 cluster on Palu:</p>
<pre>
CATEGORY         mean TIME(sec)    PERCENT    
ApplyLocPotKin   6.89E+01          16.140
Precondition     1.30E+02          30.412
Rho_comput       4.17E+01          9.761
Rho_commun       5.11E+00          1.197
PSolv_comput     1.99E+01          4.670
PSolv_commun     3.39E+01          7.938
Exchangecorr     8.10E+01          18.967
Total            4.27E+02          100
</pre>
<p>In this lesson, we will analyze the 5 most time consuming categories: ApplyLocPotKin 
(apply local Hamiltonian), Precondition, Rho_comput, Commun (all communications), and Exchangecorr. </p>

<h3>Steps to be done</h3>

<ul>
<li>      Create the input file in a given workdirectory</li>
<li>      Edit the submission script as needed by the machine used/provided.</li>
<li>      Run BigDFT jobs on the Boron 14 cluster with different combinations of ntasks and cpus-per-tasks. Note that
          we are going to calculate the speedup of the code with OpenMP threads and MPI processes, so let's choose two series 
          of jobs:
          <ol>
          <li>ntask=2, cpus-per-task=1,2, ... </li>
          <li>cpus-per-task=2, ntasks=1,2, ...</li>
          </ol>
          The speedup of the code with OpenMP threads (MPI processes) is then defined as the ratio of time required to perform a given category of operations using 1 OpenMP thread (MPI process) with the time needed to perform the same category using more than 1 OpenMP thread (MPI processes).
<u>NOTE</u>: Be aware that the code overwrites the <code>time.prc</code> file when a new job runs in a folder which already contains a <code>time.prc</code> file from a previous job. If you use different names for different jobs which write some physical data (like for example the wavefunctions), everything will be fine.</li>
<li>      Collect timing data from the file <code>time.prc</code>. We have provided a simple bash script to collect all the relevant data in the <code>time.prc</code> file. This script is is called <a href="scalability/times.bash"><code>times.bash</code></a> and is given here: 
<pre>
for i in $*
do 
rm tmp tmp2
grep -B33 'category: WFN_OPT' $i | sed  s/'Total CPU time for category: WFN_OPT'//g |\
 awk '{print $2}' > tmp
tr -s '\n' ' ' <tmp> tmp2
echo $i `cat tmp2`
done 
</pre>
It is launched like this:
<pre>
> bash times.bash time.prc
> time.prc mean 0.00E+00 4.12E+00 6.00E+00 6.33E-01 9.64E-01 6.66E-02 0.00E+00 [...]
</pre>
It basically collects all 33 timing categories inside the wavefunction optimization section of the <code>time.prc</code> file and returns them to screen. Only this section is truly relevant for timing issues because it represents the true bulk of the BigDFT code. The values described above can be extracted in the following way:
<pre>
bash times.bash time.prc | awk '{print $7,$9,$10,$11,$19,$20,$22,$34}'
</pre>
The timing values can then be copied inside a file and plotted
</li>
<li>      Plot (gnuplot, for example) figures showing the speedup with OpenMP and MPI.</li>
</ul>

      <h2>Discussions</h2>

      <p>The speedup of BigDFT on Palu is provided, as an example, in the figures on the bottom right of this page.</p>
      <img class="figureR" src="Figures/Scal_Palu.jpg" width="550" alt="Scalability measured on Palu" />
<ul>
<li>For the most time consuming categories, the MPI speedup is good</li>
<li>Communication is the main bottle-neck for MPI, but not for OpenMP. Note that this is because OpenMP works on shared memory architectures while MPI is based on distributed memory architectures.</li>
<li> The MPI speedup is slightly better than the OpenMP speedup</li>
<li>Computation of XC terms, not yet parallelized with OpenMP, is one of the reasons why OpenMP is less efficient than MPI.</li>
<li>One might wonder what would happen with GPU (which substitutes OpenMP for most operations)...
</ul>
<br><br><br><br><br><br><br><br><br><br><br>
    </div>

    <div class="footer">Author (huan D tran A unibas D ch)
      |
      <a href="http://validator.w3.org/check/referer" title="Check HTML compliance with W3C norms">XHTML1.0</a> - 
      <a href="http://jigsaw.w3.org/css-validator/check/referer" title="Check CSS compliance with W3C norms">CSS2</a>
      |
      <!-- hhmts start -->
      Last modified: Sun Oct 16 22:31:37 CEST 2011
      <!-- hhmts end -->
    </div>

  </body>
</html>
